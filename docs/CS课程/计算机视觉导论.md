---
title: è®¡ç®—æœºè§†è§‰å¯¼è®º
date: 2022-11-10 10:08:56
categories:
- CV
- courses
mathjax: true
---

æ¥è§¦è®¡ç®—æœºè§†è§‰ç›¸å…³çš„ç¬¬ä¸€é—¨è¯¾ç¨‹ã€‚

<!--more-->

## Lec 1 Introduction

### 1 CVä¸»è¦ä»»åŠ¡ï¼š

- ä¸‰ç»´é‡å»º
- å›¾åƒç†è§£
- å›¾åƒåˆæˆ

### 2 Review of Linear Algebra

çœç•¥ã€‚ã€‚ã€‚

## Lec 2 Image formation

### 1 Camera and lens

#### Pinhole camera

Add a barrier to block off most of the rays, the opening known as the aperture(å…‰åœˆ).

#### Lens

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/63.png)

Focal length:

$$\frac{1}{i}+\frac{1}{o}=\frac{1}{f}$$

Image Magnification:

$$m=\frac{h_i}{h_o}$$

Field of View (FOV):

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/64.png)

- Longer focal length = Narrower angle of view
- Shorter focal length = Wider angle of view 
- FOV also depends on sensor size

Aperture: control image brightness

F-number: represent aperture as a fraction of focal length

Depth of Field: range of object distances over which the image is sufficiently well focused

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/65.png)

### 2 Geometric image formation

#### Pinhole camera model: Perspective Projection

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/66.png)

#### Homogeneous coordinates

- Converting from Cartesian to Homogeneous coordinates(add an extra dimension)
- Converting from Homogeneous to Cartesian(remove the last dimension by dividing a number)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/67.png)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/68.png)

Homogeneous coordinates are invariant to scaling. 

Each point has an infinite set of homogeneous coordinates. 

The point in the 2D plane is projection of a ray in 3D space:

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/69.png)

#### Perspective Projection

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/70.png)

#### Orthographic projection

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/71.png)

### 3 Photometric image formation

#### Shutter speed

Shutter speed controls exposure time. The pixel value is equal to the integral of the light intensity within the exposure time.

#### Rolling shutter effect

Exposing the image line by line.

#### Color spaces

- RGB 
- HSV (Hue / Value / Saturation)

#### Color Sensing: Bayer filter

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/72.png)

## Lec 3 Image processing

### 1 Image processing basics

#### Convolution 

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/73.png)

#### 2D convolution 

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/74.png)

#### Discrete 2D convolution 

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/75.png)

This can also be the same with the vector/matrix dot product.

#### Padding 

Adding pixels around the image border.

#### Blur 

boxæ»¤æ³¢ï¼šå·ç§¯æ ¸å…¨ä¸º1

é«˜æ–¯æ»¤æ³¢ï¼šç‰¹å®šå‡½æ•°

#### Sharpen 

Sharpening is adding high frequencies. 

- Let be the original image. 
- High frequencies in image is J-blur(I), blur(I) is the low frequencied in image. 
- Sharpened image is I+(I-blur(I)).

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/76.png)

#### Gradient detection filter 

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/77.png)

#### Bilateral filter

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/78.png)

### 2 Image sampling

> Change image size / resolution. resolution: pixel / inch

#### Reducing image size 

é™é‡‡æ ·ï¼Œå»æ‰æ—è¾¹çš„åƒç´ æˆ–è€…å–å‡å€¼

#### Aliasing 

Aliasing - artifacts due to sampling 

Signals are changing too fast but sampled too slow.

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/79.png)

#### Anti-aliasing 

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/80.png)

How to do anti-aliasing 

- Convolve the image with low-pass filters (e.g. Gaussian). 
- Sample it with a Nyquist rate.

#### Fourier Transform

Represent a function as a weighted sum of sines and cosines.

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/81.png)

### 3 Image magnification

#### Interpolation 

- Nearest-neighbor interpolation 

- Linear interpolation 

- Cubic spline interpolation (Polynomial interpolation) 

  Each interval of function has different parameters

- Bilinear Interpolation (2D) Bilinear Interpolation is good enough.

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/82.png)

#### Seam Carving

A method to change aspect ratio. 

Find connected path of pixels from top to bottom of which the edge energy is minimal, removing unnoticeable pixels.

Algorithm: DP

## Lec 4 Model Fitting and Optimization

> æœ¬è®²ä¸æ•°å€¼åˆ†æè¯¾ç¨‹é«˜åº¦é‡åˆ

### 1 Optimization

> minimize f~0~(x)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/83.png)

#### Model fitting

Mean Square Error (MSE)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/84.png)

### 2 Numerical methods

Find a solution path: F (x~0~) > F (x~1~) > â€¦ > F (x~k~) > â€¦

ä¸æ•°å€¼åˆ†æè¯¾ä¸Šå†…å®¹ç±»ä¼¼ï¼Œæ³°å‹’å±•å¼€ï¼Œè¿›è¡Œè¿­ä»£ï¼Œåšä¸€é˜¶ã€äºŒé˜¶è¿‘ä¼¼

#### æ¢¯åº¦ä¸‹é™æ³•ï¼šè¿­ä»£æ³•çš„ä¸€ç§

#### Newton methodï¼šè§æ•°å€¼åˆ†æè¯¾ç¨‹

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/85.png)

#### é«˜æ–¯-ç‰›é¡¿è¿­ä»£

### 3 Robust estimation

Use other loss functions to replace MSEï¼Œå»æ‰å½±å“å¾ˆå¤§çš„å™ªç‚¹

#### Random Sample Concensus (RANSAC)

Key ideas 

- The distribution of inliers is similar  while outliers differ a lot 
- Use data point pairs to vote

###  4 Interpolation

> å…·ä½“è¯·å‚è€ƒæ•°å€¼åˆ†æ

çº¿æ€§æ’å€¼

ä¸‰æ¬¡æ ·æ¡æ’å€¼

### 5 Graphcut

#### Images as Graphs

A vertex for each pixel, an edge between each pair, each edge is weighted by the affinity or similarity between its two vertices.

#### Normalized cut

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/86.png)

#### Markov Random Field (MRF)

é©¬å°”ç§‘å¤«éšæœºåœºï¼Œæ²¡çœ‹æ‡‚

## Lec 5 Image Matching and Motion Estimation

### 1 Image matching

> Finding point-to-point correspondences between two images.

#### Steps

- Detection: Identify the interest points (key points). 
- Description: Extract vector feature descriptor surrounding each interest point. 
- Matching: Determine correspondence between descriptors in two views.

#### detection

Principal Component Analysis (PCA)

è§’ç‚¹æ£€æµ‹

- Compute the covariance matrix at each point

  ![](è®¡ç®—æœºè§†è§‰å¯¼è®º/87.png)

- Compute eigenvalues

  ![](è®¡ç®—æœºè§†è§‰å¯¼è®º/88.png)

- Classify points using eigenvalues of H:

  ![](è®¡ç®—æœºè§†è§‰å¯¼è®º/89.png)

#### Blob detector

Blobs are have large second derivatives in image intensity.

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/90.png)

#### Description

SIFT descriptorï¼šScale Invariant Feature Transform descriptor, use histogram of oriented gradients.

SIFT Algorithm

- Run DoG detector to find maximum in location/scale space.
- Find dominate orientation and normalize the orientation.
- For each (x, y, scale, orientation), create the only descriptor.

#### Matching

Define the difference between two features f1 , f2, distance L2 = ||f1 âˆ’ f2 ||

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/91.png)

### 2 Motion estimation

#### Problem

Both feature matching and motion estimation are called correspondence problems.

ç‰¹å¾è¿½è¸ªï¼š

Extract feature (interest) points and "track" them over multiple frames. 

Output: displacement of sparse points

å…‰æµæ³•ï¼š

Recover image motion at each pixel 

Output: dense displacement field (optical flow filed)

#### Lucas-Kanade Method

Key assumptionsï¼š

- Small motion: points do not move very far 
- Brightness constancy: same point looks the same(in brightness) in every frame 
- Spatial coherence: points move like their neighbors

æœ¬è®²å°æ€»ç»“ï¼š

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/92.png)

## Lec 6 Image stitching

### 1 Image Warping

Change shape of image

#### Linear Transformmation

Linear map = Matrices

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/93.png)

#### Affine Transformation

Affine map = linear map + translation

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/94.png)

Using homogenous coordinates

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/95.png)

- 6 unknowns in equations 

- 2 equations for each match 

- we need at least 3 matches to solve a affine transformation 

- for n matches, solve with least squares

  ![](è®¡ç®—æœºè§†è§‰å¯¼è®º/96.png)

The result of solution must be remembered. 

the last row with matrix must be [0 0 1]

#### Projective Transformation (Homography)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/97.png)

- 8 unknowns in equations 

- Homography matrix is up to scale (can be multiplied by a scalar), which means the degree of freedom is 8 . 

- 2 equations for each match 

- we need at least 4 matches to solve the homography 

- for n matches, solve with ||h|| = 1

  h = eigenvector of A^T^A with smallest eigenvalue

#### DoF

- Translation: The degree of freedom is 2 
- Affine: The degree of freedom is 6 
- Projective: The degree of freedom is 8

#### Forward Warping

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/98.png)

#### Inverse Warping

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/99.png)

if pixel lands between pixels, we interpolate color values from neighboring pixels.

- nearest neighbor 
- bilinear (usually this method is enough) 
- bicubic

### 2 Image Stitching

Algorithm:

- Input images 
- Feature matching 
- Compute transformation matrix with RANSAC 
- Fix image 1 and warp image 2

Cylindrical projection:

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/100.png)

## Lec 7 Structure from Motion

> Target: recover camera poses and 3D structure of a scene from its images

### 1 Camera calibration

#### å›¾åƒå¤„ç†

å‚è€ƒæ–‡çŒ®ï¼š[å›¾åƒå¤„ç†â€”â€”ç›¸æœºæ ‡å®š](https://blog.csdn.net/fengye2two/article/details/80686409/)

> ä¸–ç•Œåæ ‡ç³»ï¼ˆworld coordinateï¼‰(*xw,yw,zw*)ï¼Œä¹Ÿç§°ä¸ºæµ‹é‡åæ ‡ç³»ï¼Œæ˜¯ä¸€ä¸ªä¸‰ç»´ç›´è§’åæ ‡ç³»ï¼Œä»¥å…¶ä¸ºåŸºå‡†å¯ä»¥æè¿°ç›¸æœºå’Œå¾…æµ‹ç‰©ä½“çš„ç©ºé—´ä½ç½®ã€‚ä¸–ç•Œåæ ‡ç³»çš„ä½ç½®å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè‡ªç”±ç¡®å®šã€‚ä¸–ç•Œåæ ‡ç³»çš„æœ€å°å•ä½ä¸ºmmã€‚

> ç›¸æœºåæ ‡ç³»ï¼ˆcamera coordinateï¼‰(*xc,yc,zc*)ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªä¸‰ç»´ç›´è§’åæ ‡ç³»ï¼ŒåŸç‚¹ä½äºé•œå¤´å…‰å¿ƒå¤„ï¼Œxcã€ycè½´åˆ†åˆ«ä¸åƒé¢çš„ä¸¤è¾¹å¹³è¡Œï¼Œzcè½´ä¸ºé•œå¤´å…‰è½´ï¼Œä¸åƒå¹³é¢å‚ç›´ã€‚ç›¸æœºåæ ‡ç³»çš„æœ€å°å•ä½ä¸ºmmã€‚

>å›¾åƒåæ ‡ç³»ï¼ˆimage coordinateï¼‰(*x*,*y*)ï¼Œæ˜¯åƒå¹³é¢ä¸Šçš„äºŒç»´ç›´è§’åæ ‡ç³»ã€‚å›¾åƒåæ ‡ç³»çš„åŸç‚¹ä¸ºé•œå¤´å…‰è½´ä¸åƒå¹³é¢çš„äº¤ç‚¹ï¼ˆä¹Ÿç§°ä¸»ç‚¹ï¼Œprincipal pointï¼‰ï¼Œå®ƒçš„xè½´ä¸ç›¸æœºåæ ‡ç³»çš„xcè½´å¹³è¡Œï¼Œå®ƒçš„yè½´ä¸ç›¸æœºåæ ‡ç³»çš„ycè½´å¹³è¡Œã€‚å›¾åƒåæ ‡ç³»çš„æœ€å°å•ä½ä¸ºmmã€‚

>åƒç´ åæ ‡ç³»ï¼ˆpixel coordinateï¼‰(u,v)ï¼Œæ˜¯å›¾åƒå¤„ç†å·¥ä½œä¸­å¸¸ç”¨çš„äºŒç»´ç›´è§’åæ ‡ç³»ï¼Œåæ˜ äº†ç›¸æœºCCD/CMOSèŠ¯ç‰‡ä¸­åƒç´ çš„æ’åˆ—æƒ…å†µã€‚å®ƒçš„åŸç‚¹ä½äºå›¾åƒå·¦ä¸Šè§’ï¼Œæ¨ªåæ ‡uè¡¨ç¤ºåƒç´ æ‰€åœ¨çš„åˆ—ï¼Œçºµåæ ‡vè¡¨ç¤ºåƒç´ æ‰€åœ¨çš„è¡Œã€‚åƒç´ åæ ‡ç³»ä¸å›¾åƒåæ ‡ç³»å¯ä»¥ç®€å•ç†è§£ä¸ºå¹³ç§»å…³ç³»ï¼Œå®ƒä»¬åŒå¤„äºåƒå¹³é¢ã€‚åƒç´ åæ ‡ç³»çš„xè½´ä¸å›¾åƒåæ ‡ç³»çš„uè½´å¹³è¡Œï¼Œåƒç´ åæ ‡ç³»çš„yè½´ä¸å›¾åƒåæ ‡ç³»çš„vè½´å¹³è¡Œã€‚åƒç´ åæ ‡ç³»çš„æœ€å°å•ä½ä¸ºåƒç´ ã€‚

å˜æ¢è¿‡ç¨‹ï¼š

ä¸–ç•Œ=ã€‹ç›¸æœº=ã€‹å›¾åƒ=ã€‹åƒç´ 

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/1.png)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/2.png)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/3.png)

so, it is similar to lab2.

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/4.png)

ä¸–ç•Œç›´æ¥è½¬æ¢ä¸ºåƒç´ ï¼š

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/5.png)

è§£æ–¹ç¨‹æ—¶ï¼š

- æ‰¾ç‰¹å¾ç‚¹ï¼Œå»ºç«‹æ–¹ç¨‹æ±‚è§£æœªçŸ¥æ•°$p$

> å…·ä½“æŸ¥çœ‹å‚è€ƒæ–‡çŒ®å’Œè¯¾ç¨‹PPT

#### PnPé—®é¢˜

å‚è€ƒæ–‡çŒ®ï¼š[PnPé—®é¢˜å„ç§ç®—æ³•æ€»ç»“åˆ†æ](https://zhuanlan.zhihu.com/p/399140251)

> é—®é¢˜æè¿°ï¼šå·²çŸ¥nä¸ª3Dç‚¹çš„åæ ‡(ç›¸å¯¹ä¸–ç•Œåæ ‡ç³»)ä»¥åŠè¿™äº›ç‚¹çš„åƒç´ åæ ‡æ—¶ï¼Œå¦‚ä½•ä¼°è®¡ç›¸æœºçš„ä½å§¿

##### Direct Linear Transform (DLT)

å‰é¢æˆ‘ä»¬é€šè¿‡è§£æ–¹ç¨‹çš„å½¢å¼è§£å‡ºäº†è¿™ä¸ªæ–¹ç¨‹ï¼Œè¿™ç§æ–¹æ³•å°±å«åšDLTã€‚

##### P3P

è‡³å°‘ä¸‰ä¸ªå¯¹åº”å…³ç³»å¯ä»¥è§£å‡ºç›¸æœºåæ ‡ï¼Œè¿˜éœ€è¦ä¸€ä¸ªå¯¹åº”å…³ç³»ä½¿è¿™ä¸ªè§£æ˜¯ç‰¹è§£ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/6.png)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/7.png)

##### EPnP

Main steps: 

1. Represent each point as the linear combination of 4 control points c~i~. 
2. Construct a linear system in the control-point coordinate.

3. Solve the equation.

### 2 Two-frame structure from motion

1. Assume Camera Matrix ğ¾ is known for each camera 
2. Find a few Reliable Corresponding Points
3. Find Relative Camera Position ğ­ and Orientation ğ‘…
4. Find 3D position of scene points

è¯¦ç»†è®²è§£ï¼š[å¯¹æå‡ ä½•--çŸ¥ä¹](https://zhuanlan.zhihu.com/p/472205819)

â€‹					[å¯¹æå‡ ä½•--github](https://xhy3054.github.io/epipolar-geometry/)

### 3 Multi-frame structure from motion

1. Initialize camera motion and scene structure 
2. For each additional view - Determine projection matrix of new camera using all the
    known 3D points that are visible in its image - Refine and extend structure: compute new 3D points, reoptimize existing points that are also seen by this camera
3. Refine structure and motion: Bundle Adjustment

### 4 A modern SfM system: COLMAP

> sfM: Structure-from-Motion
>
> MVS: Multi-View Stereo

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/8.png)

## Lec 8 Depth estimation and 3D reconstruction

### 1 Depth estimation

#### 1.1 Introduction

â€‹	æ·±åº¦ä¼ æ„Ÿå™¨é¡¾åæ€ä¹‰æ˜¯ç”¨æ¥æ¢æµ‹ç¯å¢ƒç‰©ä½“ä¸ä¼ æ„Ÿå™¨ä¹‹é—´çš„è·ç¦»çš„ã€‚å®ƒçš„è¾“å‡ºä¸»è¦å¯ä»¥è¡¨ç¤ºä¸ºæ·±åº¦å›¾(depth map)å’Œç‚¹äº‘(point cloud)è¿™ä¸¤ç§å½¢å¼ã€‚

â€‹	æ·±åº¦å›¾åƒï¼ˆdepth image)ä¹Ÿè¢«ç§°ä¸ºè·ç¦»å½±åƒï¼ˆrange imageï¼‰ï¼Œæ˜¯æŒ‡å°†ä»å›¾åƒé‡‡é›†å™¨åˆ°åœºæ™¯ä¸­å„ç‚¹çš„è·ç¦»ï¼ˆæ·±åº¦ï¼‰ä½œä¸ºåƒç´ å€¼çš„å›¾åƒï¼Œå®ƒç›´æ¥åæ˜ äº†æ™¯ç‰©å¯è§è¡¨é¢çš„å‡ ä½•å½¢çŠ¶ã€‚æ·±åº¦å›¾åƒç»è¿‡åæ ‡è½¬æ¢å¯ä»¥è®¡ç®—ä¸ºç‚¹äº‘æ•°æ®ï¼Œæœ‰è§„åˆ™åŠå¿…è¦ä¿¡æ¯çš„ç‚¹äº‘æ•°æ®ä¹Ÿå¯ä»¥åç®—ä¸ºæ·±åº¦å›¾åƒæ•°æ®ã€‚
æ·±åº¦æ•°æ®æµæ‰€æä¾›çš„å›¾åƒå¸§ä¸­ï¼Œæ¯ä¸€ä¸ªåƒç´ ç‚¹ä»£è¡¨çš„æ˜¯åœ¨æ·±åº¦æ„Ÿåº”å™¨çš„è§†é‡ä¸­ï¼Œè¯¥ç‰¹å®šçš„ï¼ˆx, yï¼‰åæ ‡å¤„ç‰©ä½“åˆ°ç¦»æ‘„åƒå¤´å¹³é¢æœ€è¿‘çš„ç‰©ä½“åˆ°è¯¥å¹³é¢çš„è·ç¦»ï¼ˆä»¥æ¯«ç±³ä¸ºå•ä½ï¼‰ã€‚

- è¢«åŠ¨æµ‹è·ä¼ æ„Ÿ(Passive depth sensing)

> è¢«åŠ¨æµ‹è·ä¼ æ„Ÿ=ä¸¤ä¸ªç›¸éš”ä¸€å®šè·ç¦»çš„ç›¸æœºè·å¾—ä¸¤å¹…å›¾åƒ+ç«‹ä½“åŒ¹é…+ä¸‰è§’åŸç†è®¡ç®—è§†å·®ï¼ˆdisparityï¼‰

â€‹		ä¸¤ä¸ªç›¸éš”ä¸€å®šè·ç¦»çš„æ‘„åƒæœºåŒæ—¶è·å–åŒä¸€åœºæ™¯çš„ä¸¤å¹…å›¾åƒï¼Œé€šè¿‡ç«‹ä½“åŒ¹é…ç®—æ³•æ‰¾åˆ°ä¸¤å¹…å›¾åƒä¸­å¯¹åº”çš„åƒç´ ç‚¹ï¼Œéšåæ ¹æ®ä¸‰è§’åŸç†è®¡ç®—å‡ºè§†å·®ä¿¡æ¯ï¼Œè€Œè§†å·®ä¿¡æ¯é€šè¿‡è½¬æ¢å¯ç”¨äºè¡¨å¾åœºæ™¯ä¸­ç‰©ä½“çš„æ·±åº¦ä¿¡æ¯ã€‚åŸºäºç«‹ä½“åŒ¹é…ç®—æ³•ï¼Œè¿˜å¯é€šè¿‡æ‹æ‘„åŒä¸€åœºæ™¯ä¸‹ä¸åŒè§’åº¦çš„ä¸€ç»„å›¾åƒæ¥è·å¾—è¯¥åœºæ™¯çš„æ·±åº¦å›¾åƒã€‚é™¤æ­¤ä¹‹å¤–ï¼Œåœºæ™¯æ·±åº¦ä¿¡æ¯è¿˜å¯ä»¥é€šè¿‡å¯¹å›¾åƒçš„å…‰åº¦ç‰¹å¾ã€æ˜æš—ç‰¹å¾ç­‰ç‰¹å¾è¿›è¡Œåˆ†æé—´æ¥ä¼°ç®—å¾—åˆ°ã€‚

- ä¸»åŠ¨æµ‹è·ä¼ æ„Ÿ(Active depth sensing)

â€‹		ä¸»åŠ¨æµ‹è·ä¼ æ„Ÿç›¸æ¯”è¾ƒäºè¢«åŠ¨æµ‹è·ä¼ æ„Ÿæœ€æ˜æ˜¾çš„ç‰¹å¾æ˜¯ï¼šè®¾å¤‡æœ¬èº«éœ€è¦å‘å°„èƒ½é‡æ¥å®Œæˆæ·±åº¦ä¿¡æ¯çš„é‡‡é›†ã€‚è¿™ä¹Ÿå°±ä¿è¯äº†æ·±åº¦å›¾åƒçš„è·å–ç‹¬ç«‹äºå½©è‰²å›¾åƒçš„è·å–ã€‚è¿‘å¹´æ¥ï¼Œä¸»åŠ¨æ·±åº¦ä¼ æ„Ÿåœ¨å¸‚é¢ä¸Šçš„åº”ç”¨æ„ˆåŠ ä¸°å¯Œã€‚ä¸»åŠ¨æ·±åº¦ä¼ æ„Ÿçš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬äº†TOFï¼ˆTime of Flightï¼‰ã€ç»“æ„å…‰ã€æ¿€å…‰æ‰«æç­‰ã€‚

#### 1.2 Stereo matching

> å‚è€ƒèµ„æ–™[3Dè§†è§‰ä¹‹ç«‹ä½“åŒ¹é…](https://zhuanlan.zhihu.com/p/161276985)
>
> [ç«‹ä½“åŒ¹é…ç®—æ³•](https://blog.csdn.net/Android_WPF/article/details/126434543)

æœ€ç®€å•çš„ç®—æ³•ï¼š

- For each pixel in the first image 
  - Find corresponding epipolar line in the right image
  - Search along epipolar line and pick the best match
- Simplest case: epipolar lines are horizontal scanlines

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/9.png)

è¿™æ ·å°±æ‰¾åˆ°äº†ä¸¤ä¸ªç›¸åŒçš„ç‚¹ï¼Œç„¶åè®¡ç®—æ·±åº¦ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/10.png)

å¦‚æœè§†è§’ä¸åœ¨åŒä¸€æ°´å¹³çº¿ä¸Šï¼Œå°±å…ˆæŠŠä»–ä»¬è½¬åˆ°åŒä¸€æ°´å¹³çº¿ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/11.png)

Stereo as energy minimizationï¼šè®©å½“å‰åƒç´ çš„ä»£ä»·èšåˆè¿‡ç¨‹å—å¤šä¸ªæ–¹å‘(æˆ–è·¯å¾„)ä¸Šæ‰€æœ‰åƒç´ çš„å½±å“ï¼Œæ–¹å‘è¶Šå¤šå‚ä¸å½±å“å½“å‰åƒç´ çš„é‚»åŸŸåƒç´ å°±è¶Šå¤š

åŠ¨æ€è§„åˆ’ï¼š

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/12.png)

Choosing the stereo baselineï¼š

- Too small: large depth error 
- Too large: difficult search problem

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/13.png)

#### 1.3 Multi-view stereo

Plane-Sweep: [å¹³é¢æ‰«æç®—æ³•](https://blog.csdn.net/xuangenihao/article/details/81392684)

PatchMatch: [PatchMatch](PatchMatch)

1. Initialize pixels with random patch offsets
2. Check if neighbors have better patch offsets
3. Search in concentric radius around the current offset for better better patch offsets
4. Go to Step 2 until converge.

### 2 3D reconstruction

#### 2.1 3D representations

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/14.png)

- ç‚¹äº‘

- mesh ç”¨G(E, V)è¡¨ç¤º

- voxel

  ![](è®¡ç®—æœºè§†è§‰å¯¼è®º/15.png)

- SDF(Signed Distance Function)

  - The distance of a point to the shape boundary
  - The distance is defined by a metric, usually the Euclidean distance

  Truncated Signed Distance Function (TSDF): Truncation SDFâ€™s distance value to [âˆ’1, 1]

#### 2.2 3D surface reconstruction

[KinectFusion](https://blog.csdn.net/qinqinxiansheng/article/details/119449196)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/16.png)

**[æ³Šæ¾é‡å»º](https://www.jianshu.com/p/8641e0db0367)**

[**Marching Cubesç®—æ³•**](https://blog.csdn.net/weixin_38060850/article/details/109143025)

è§†é¢‘ä»‹ç»Marching Cubesç®—æ³•: 

<iframe src="//player.bilibili.com/player.html?aid=79262663&bvid=BV1yJ411r73v&cid=135644481&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

[Marching Squares](https://blog.csdn.net/whuawell/article/details/74998280) åŸºæœ¬å’ŒMarching cubes ç±»ä¼¼ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/17.png)

**[COLMAP](https://blog.csdn.net/jiankangyq/article/details/121808174)**: ä¸€ç§é€šç”¨çš„è¿åŠ¨ç»“æ„ (SfM) å’Œå¤šè§†å›¾ç«‹ä½“ (MVS) ç®¡é“ã€‚

#### 2.3 Texture mapping

> Surface lives in 3D world space
>
> Every 3D surface point also has a place where it goes in the 2D image (texture).

[çº¹ç†æ˜ å°„(Texture mapping)](https://zhuanlan.zhihu.com/p/364045620)

## Lec 9 Deep Learning

### 1 Machine learning

> ä¼ ç»Ÿç¨‹åºæ˜¯ç»™ç”µè„‘è¾“å…¥å’Œç¨‹åºï¼Œç”µè„‘ç»™å‡ºè¾“å‡ºã€‚
>
> æœºå™¨å­¦ä¹ æ˜¯ç»™ç”µè„‘è¾“å…¥å’Œè¾“å‡ºï¼Œç”µè„‘ç»™å‡ºç¨‹åºã€‚

#### æ¦‚å¿µ

- Model: $x$å’Œ$y$ä¹‹é—´å…³ç³»çš„æ•°å­¦è¡¨ç¤º

- Supervised learning(ç›‘ç£å­¦ä¹ ): å¯ä»¥ç”±è®­ç»ƒèµ„æ–™ä¸­å­¦åˆ°æˆ–å»ºç«‹ä¸€ä¸ªæ¨¡å¼ï¼ˆå‡½æ•°/learning modelï¼‰ï¼Œå¹¶ä¸”ä¾æ¬¡æ¨¡å¼æ¨æµ‹å‡ºæ–°çš„å®ä¾‹ã€‚

  labeled data: exisitng (x,y) pairs, called training data.

- æœºå™¨å­¦ä¹ çš„ä¸¤ä¸ªé˜¶æ®µï¼š

  - è®­ç»ƒ(Training)
  - æµ‹è¯•(Testing)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/18.png)

### 2 Linear classifier

#### CLassification model

> è¾“å…¥æ˜¯ä¸€å¼ å›¾ç‰‡
>
> è¾“å‡ºæ˜¯æ¯ä¸ªåˆ†ç±»çš„å¯¹åº”åˆ†æ•°

æœ‰ä¸¤éƒ¨åˆ†ç»„æˆï¼š

- è¯„åˆ†å‡½æ•°
- æŸå¤±å‡½æ•°

#### Linear classifier

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/19.png)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/20.png)

å°†ä¸€å¼ ç…§ç‰‡é‡Œé¢çš„æ‰€æœ‰åƒç´ å˜æˆä¸€ä¸ªå‘é‡ã€‚

$f(x_i,W,b) = Wx_i + b$

å‚æ•°**W**è¢«ç§°ä¸º**æƒé‡ï¼ˆweightsï¼‰**ï¼Œ**b**è¢«ç§°ä¸º**åå·®å‘é‡ï¼ˆbias vectorï¼‰**ã€‚

- é¦–å…ˆï¼Œä¸€ä¸ªå•ç‹¬çš„çŸ©é˜µä¹˜æ³•$Wx_i$å°±é«˜æ•ˆåœ°å¹¶è¡Œè¯„ä¼°10ä¸ªä¸åŒçš„åˆ†ç±»å™¨ï¼ˆæ¯ä¸ªåˆ†ç±»å™¨é’ˆå¯¹ä¸€ä¸ªåˆ†ç±»ï¼‰ï¼Œå…¶ä¸­æ¯ä¸ªç±»çš„åˆ†ç±»å™¨å°±æ˜¯Wçš„ä¸€ä¸ªè¡Œå‘é‡ã€‚
- è®­ç»ƒæ•°æ®ç”¨æ¥å­¦ä¹ $W$å’Œ$b$
- ä¸€å¼ å›¾åƒå¯çœ‹åšé«˜ç»´ç©ºé—´çš„ä¸€ä¸ªç‚¹ï¼Œæ¯ä¸ªåˆ†ç±»å°±æ˜¯æŠŠè¿™äº›ç‚¹åˆ’åˆ†æˆè‹¥å¹²ä¸ªåŒºåŸŸã€‚

#### Loss function

> åˆ¤æ–­ä¸€ä¸ªæƒé‡çŸ©é˜µæ˜¯å¦è¶³å¤Ÿå¥½

> å›å½’é—®é¢˜ä½¿ç”¨å‡æ–¹è¯¯å·®(MSE)
>
> åˆ†ç±»é—®é¢˜ä½¿ç”¨äº¤å‰ç†µ(Cross Entropy Loss)
>
> å‚è€ƒèµ„æ–™ï¼š[ç®€å•è°ˆè°ˆCross Entropy Loss](https://blog.csdn.net/xg123321123/article/details/80781611)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/21.png)

Softmax: æŠŠKä¸ªå®å€¼è½¬æ¢ä¸ºå¦å¤–Kä¸ªå®å€¼å¹¶ä½¿Kä¸ªå®å€¼ä¹‹å’Œä¸º1çš„å‡½æ•°ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/22.png)

### 3 Neural networks

> å‚è€ƒèµ„æ–™ï¼š[æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰](https://blog.csdn.net/weixin_39910711/article/details/114849349)

â€‹	**æ¿€æ´»å‡½æ•°**ï¼šä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°çš„è¯ï¼Œç¥ç»ç½‘ç»œçš„æ¯å±‚éƒ½åªæ˜¯åš**çº¿æ€§å˜æ¢**ï¼Œå¤šå±‚è¾“å…¥å åŠ åä¹Ÿè¿˜æ˜¯çº¿æ€§å˜æ¢ã€‚å› ä¸ºçº¿æ€§æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›é€šå¸¸ä¸å¤Ÿï¼Œæ‰€ä»¥è¿™æ—¶å€™å°±ä½“ç°äº†æ¿€æ´»å‡½æ•°çš„ä½œç”¨äº†ï¼Œæ¿€æ´»å‡½æ•°å¯ä»¥å¼•å…¥**éçº¿æ€§å› ç´ **ã€‚

â€‹	åœ¨ç¥ç»ç½‘ç»œæ¯ä¸€å±‚ç¥ç»å…ƒåšå®Œçº¿æ€§å˜æ¢åï¼ŒåŠ ä¸Šä¸€ä¸ªéçº¿æ€§æ¿€åŠ±å‡½æ•°å¯¹çº¿æ€§å˜æ¢çš„ç»“æœè¿›è¡Œè½¬æ¢ï¼Œè¾“å‡ºå°±å¯ä»¥å˜æˆä¸€ä¸ªéçº¿æ€§çš„å‡½æ•°ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/23.png)

**å¤šå±‚æ„ŸçŸ¥å™¨**

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/24.png)

**å…¨è¿æ¥ç¥ç»ç½‘ç»œ**

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/25.png)

### 4 Convolutional neural networks--å·ç§¯ç¥ç»ç½‘ç»œ

> å‚è€ƒèµ„æ–™ï¼š[å·ç§¯ç¥ç»ç½‘ç»œè¶…è¯¦ç»†ä»‹ç»](https://blog.csdn.net/jiaoyangwm/article/details/80011656/)

#### Convolution = local connectivity + weight sharing

> ä¸¤è€…çš„å…³é”®ä½œç”¨å°±æ˜¯å‡å°‘å‚æ•°æ•°é‡ï¼Œä½¿è¿ç®—å˜å¾—ç®€æ´ã€é«˜æ•ˆï¼Œèƒ½å¤Ÿåœ¨è¶…å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿ç®—

local connectivity(å±€éƒ¨è¿æ¥): å¯¹äºå±€éƒ¨è¿æ¥è€Œè¨€ï¼šå±‚é—´ç¥ç»åªæœ‰å±€éƒ¨èŒƒå›´å†…çš„è¿æ¥ï¼Œåœ¨è¿™ä¸ªèŒƒå›´å†…é‡‡ç”¨å…¨è¿æ¥çš„æ–¹å¼ï¼Œè¶…è¿‡è¿™ä¸ªèŒƒå›´çš„ç¥ç»å…ƒåˆ™æ²¡æœ‰è¿æ¥ï¼›è¿æ¥ä¸è¿æ¥ä¹‹é—´ç‹¬ç«‹å‚æ•°ï¼Œç›¸æ¯”äºå»å…¨è¿æ¥å‡å°‘äº†æ„Ÿå—åŸŸå¤–çš„è¿æ¥ï¼Œæœ‰æ•ˆå‡å°‘å‚æ•°è§„æ¨¡ã€‚

weight sharing(æƒå€¼å…±äº«): ä»å›¾åƒå±€éƒ¨å­¦ä¹ åˆ°çš„ä¿¡æ¯åº”ç”¨åˆ°å›¾åƒçš„å…¶ä»–éƒ¨ä½å»ã€‚æƒå€¼å…±äº«æ„å‘³ç€æ¯ä¸€ä¸ªè¿‡æ»¤å™¨åœ¨éå†æ•´ä¸ªå›¾åƒçš„æ—¶å€™ï¼Œè¿‡æ»¤å™¨çš„å‚æ•°(å³è¿‡æ»¤å™¨çš„å‚æ•°çš„å€¼)æ˜¯å›ºå®šä¸å˜çš„

å‚è€ƒï¼š[weight sharing](https://blog.csdn.net/malvas/article/details/86647781)

#### æ„Ÿå—é‡(Receptive fields)

- è‹¥ç›®æ ‡ç›¸å¯¹æ„Ÿå—é‡è¿‡å°ï¼Œé‚£è®­ç»ƒå‚æ•°åªæœ‰å°‘éƒ¨åˆ†æ˜¯å¯¹åº”äºè®­ç»ƒç›®æ ‡çš„ï¼Œåˆ™åœ¨æµ‹è¯•ç¯èŠ‚ï¼Œä¹Ÿå¾ˆéš¾æ£€æµ‹å‡ºç±»ä¼¼çš„ç›®æ ‡ï¼›
- è‹¥ç›®æ ‡ç›¸å¯¹æ„Ÿå—é‡è¿‡å¤§ï¼Œé‚£è®­ç»ƒçš„å‚æ•°éƒ½æ˜¯å¯¹åº”äºæ•´ä¸ªå¯¹è±¡çš„å±€éƒ¨ä¿¡æ¯ï¼Œæ˜¯ä¸å¤Ÿåˆ©äºæ£€æµ‹å°ç›®æ ‡çš„ã€‚

#### æ± åŒ–å±‚(Pooling layer)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/26.png)

#### æ€»ä½“æ¡†æ¶

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/27.png)

### 5 Training neural networks

#### æ¢¯åº¦ä¸‹é™è®­ç»ƒCNN

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/28.png)

#### åå‘ä¼ æ’­

> å‚è€ƒï¼š[åå‘ä¼ æ’­-cs231n](https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit)

1. Forward data through the network, get loss
2. Backprop to calculate the gradients
3. Update the parameters using the gradient
4. Go to step 1 if not converged

#### éšæœºæ¢¯åº¦ä¸‹é™æ³•(SGD)

> å‚è€ƒï¼š[éšæœºæ¢¯åº¦ä¸‹é™è¯¦è§£](https://blog.csdn.net/weixin_41803874/article/details/114016587)

ä»…è®¡ç®—ä¸€æ‰¹éšæœºé‡‡æ ·å›¾åƒä¸Šçš„æŸå¤±å’Œæ¢¯åº¦ã€‚

#### è¶…å‚æ•°(hyper-parameters)

ç®—æ³•è¿è¡Œå‰éœ€è¦å†³å®šçš„å‚æ•°ã€‚

é€‰æ‹©ä¾æ®ï¼š

1. Train for original model
2. Validate to find hyperparameters
3. Test to understand generalizability

#### è¿‡æ‹Ÿåˆ(overfitting)

æŠŠå™ªéŸ³ç‚¹ä¹Ÿæ‹Ÿåˆä¸Šäº†ï¼Œè¿‡åˆ†ä¾èµ–æ•°æ®é›†ã€‚

é˜²æ­¢ï¼š

1. Cross validation(éªŒè¯) and early stop

2. Regularization(æ­£åˆ™åŒ–) or dropout

   **æ­£åˆ™åŒ–**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­ç»™æ¯ä¸ªå‚æ•° w åŠ ä¸Šæƒé‡ï¼Œå¼•å…¥æ¨¡å‹å¤æ‚åº¦æŒ‡æ ‡ï¼Œä»è€ŒæŠ‘åˆ¶æ¨¡å‹å™ªå£°ï¼Œå‡å°è¿‡æ‹Ÿåˆã€‚ 

   **dropout**ï¼šåœ¨å‰å‘ä¼ æ’­çš„æ—¶å€™ï¼Œè®©æŸä¸ªç¥ç»å…ƒçš„æ¿€æ´»å€¼ä»¥ä¸€å®šçš„æ¦‚ç‡påœæ­¢å·¥ä½œï¼Œè¿™æ ·å¯ä»¥ä½¿æ¨¡å‹æ³›åŒ–æ€§æ›´å¼ºï¼Œå› ä¸ºå®ƒä¸ä¼šå¤ªä¾èµ–æŸäº›å±€éƒ¨çš„ç‰¹å¾

3. Data augmentation(æ•°æ®å¢å¼º)

   > å‚è€ƒï¼š[æ•°æ®å¢å¼º(Data Augmentation)](https://zhuanlan.zhihu.com/p/41679153)

   â€‹	ä¸ºäº†è·å¾—æ›´å¤šçš„æ•°æ®ï¼Œæˆ‘ä»¬åªè¦å¯¹ç°æœ‰çš„æ•°æ®é›†è¿›è¡Œå¾®å°çš„æ”¹å˜ã€‚æ¯”å¦‚æ—‹è½¬ï¼ˆflipsï¼‰ã€ç§»ä½ï¼ˆtranslationsï¼‰ã€æ—‹è½¬ï¼ˆrotationsï¼‰ç­‰å¾®å°çš„æ”¹å˜ã€‚æˆ‘ä»¬çš„ç½‘ç»œä¼šè®¤ä¸ºè¿™æ˜¯ä¸åŒçš„å›¾ç‰‡ã€‚

#### æ‰¹æ ‡å‡†åŒ–(Batch Normalization)

> å‚è€ƒ[æ·±å…¥ç†è§£BN](https://www.cnblogs.com/guoyaohua/p/8724433.html)

â€‹	ç›®çš„ï¼šReduce internal covariate shift([å†…éƒ¨åå˜é‡åç§»](https://zhuanlan.zhihu.com/p/480425962))

â€‹	ç¥ç»ç½‘ç»œçš„æ·±åº¦å¢åŠ ï¼Œæ¯å±‚ç‰¹å¾å€¼åˆ†å¸ƒä¼šé€æ¸çš„å‘æ¿€æ´»å‡½æ•°çš„è¾“å‡ºåŒºé—´çš„ä¸Šä¸‹ä¸¤ç«¯ï¼ˆæ¿€æ´»å‡½æ•°é¥±å’ŒåŒºé—´ï¼‰é è¿‘ï¼Œè¿™æ ·ç»§ç»­ä¸‹å»å°±ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚BNå°±æ˜¯é€šè¿‡æ–¹æ³•å°†**è¯¥å±‚ç‰¹å¾å€¼åˆ†å¸ƒé‡æ–°æ‹‰å›æ ‡å‡†æ­£æ€åˆ†å¸ƒ**ï¼Œç‰¹å¾å€¼å°†è½åœ¨æ¿€æ´»å‡½æ•°å¯¹äºè¾“å…¥è¾ƒä¸ºæ•æ„Ÿçš„åŒºé—´ï¼Œè¾“å…¥çš„å°å˜åŒ–å¯å¯¼è‡´æŸå¤±å‡½æ•°è¾ƒå¤§çš„å˜åŒ–ï¼Œä½¿å¾—æ¢¯åº¦å˜å¤§ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±ï¼ŒåŒæ—¶ä¹Ÿå¯åŠ å¿«æ”¶æ•›ã€‚

â€‹	è®­ç»ƒæ—¶çš„ä½¿ç”¨æ–¹æ³•ï¼šå¯¹æ¯ä¸ªéšå±‚åŠ ä¸Šä¸€å±‚BNã€‚

### 6 Network architectures

> ä»¥å‰å‘å±•ä¸å¥½ï¼š
>
> - æ•°æ®é›†è¿‡å°å¯¼è‡´è¿‡æ‹Ÿåˆ
> - è®¡ç®—èƒ½åŠ›ä¸å¤Ÿ

[AlexNet](https://blog.csdn.net/qq_42076902/article/details/123864381)

[ResNet](https://blog.csdn.net/qq_45649076/article/details/120494328)

[DenseNet](https://blog.csdn.net/qq_44766883/article/details/112011420)ï¼šäº’ç›¸è¿æ¥æ‰€æœ‰çš„å±‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/29.png)

[MobileNets](https://blog.csdn.net/qq_47233366/article/details/123029998)

[Neural Architecture Search](https://blog.csdn.net/fengmaomao1991/article/details/121247163)(ç¥ç»æ¶æ„æœç´¢)

## Lec 10 Recognition

### 1 Semantic segmentation(è¯­ä¹‰åˆ†å‰²)

> åœ¨å›¾åƒé¢†åŸŸï¼Œè¯­ä¹‰æŒ‡çš„æ˜¯å›¾åƒçš„å†…å®¹ï¼Œå¯¹å›¾ç‰‡æ„æ€çš„ç†è§£ï¼Œæ¯”å¦‚å·¦å›¾çš„è¯­ä¹‰å°±æ˜¯ä¸‰ä¸ªäººéª‘ç€ä¸‰è¾†è‡ªè¡Œè½¦ï¼›åˆ†å‰²çš„æ„æ€æ˜¯ä»åƒç´ çš„è§’åº¦åˆ†å‰²å‡ºå›¾ç‰‡ä¸­çš„ä¸åŒå¯¹è±¡ï¼Œå¯¹åŸå›¾ä¸­çš„æ¯ä¸ªåƒç´ éƒ½è¿›è¡Œæ ‡æ³¨ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/30.png)

#### åè¯è§£é‡Š

æ»‘åŠ¨çª—å£ï¼šæ—¶é—´å¤æ‚åº¦é«˜ï¼Œæœ‰é™çš„æ„Ÿå—é‡ã€‚

å…¨è¿æ¥å·ç§¯ç½‘ç»œï¼šä¸€æ¬¡åšå‡ºé¢„æµ‹ï¼ŒæŸå¤±å‡½æ•°æ˜¯æ¯ä¸ªåƒç´ çš„äº¤å‰ç†µã€‚

Unpollingï¼šä¸€ç§ä¸Šé‡‡æ ·æ–¹æ³•ï¼Œæœ‰å¾ˆå¤šç§å…·ä½“æ¡ˆä¾‹ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/31.png)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/32.png)

#### U-Net

Skip Connection: è·³è¿‡ä¸­é—´è¿æ¥ï¼Œä½¿æ·±å±‚å’Œæµ…å±‚è¿æ¥èµ·æ¥ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/33.png)

#### DeepLab

> å‚è€ƒ: [å›¾åƒè¯­ä¹‰åˆ†å‰²ä¹‹FCNå’ŒCRF](https://blog.csdn.net/weixin_42137700/article/details/81835354)

å›¾åƒè¯­ä¹‰åˆ†å‰²æ­¥éª¤ï¼š

- FCN - å…¨å·ç§¯ç½‘ç»œ
- CRF - æ¡ä»¶éšæœºåœº(Conditional random field)
- MRF - é©¬å°”ç§‘å¤«éšæœºåœº

#### è¯„ä¼°æŒ‡æ ‡

Per-pixel Intersection-over-union

### 2 Object detection(ç›®æ ‡æ£€æµ‹)

> è¾“å…¥ï¼šä¸€å¼ RGBå›¾ç‰‡
>
> è¾“å‡ºï¼šè¡¨ç¤ºå¯¹è±¡çš„ä¸€ç»„è¾¹ç•Œæ¡†(ç±»åˆ«æ ‡ç­¾ã€æ¡†çš„ä½ç½®ï¼Œæ¡†çš„å¤§å°)

#### å•ä¸ªç‰©ä½“æ£€æµ‹

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/34.png)

#### å¤šä¸ªç‰©ä½“æ£€æµ‹

ä¸€å¼ ç…§ç‰‡ç»è¿‡å„ç§ä¸åŒçš„ç¥ç»ç½‘ç»œï¼Œå¾—å‡ºç»“æœã€‚

#### æ»‘åŠ¨çª—å£

Apply a CNN to many different crops of the image, CNN classifies each crop as object or background.

ä¸€å¼ å›¾ç‰‡å¯ä»¥è¢«æ‹†åˆ†æˆå¾ˆå¤šboxesï¼Œæˆ‘ä»¬ä¸èƒ½æ£€æµ‹æ‰€æœ‰è¿™äº›å›¾ç‰‡ã€‚

#### Region proposals(å€™é€‰åŒºåŸŸ)

ç”¨å›¾åƒåˆ†å‰²ç®—æ³•å…ˆåˆ†å‰²å›¾åƒï¼Œç„¶åå†è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚

#### R-CNN

1. é€‰å»ºè®®æ¡†å¹¶è°ƒæ•´å°ºå¯¸
2. å¯¹æ¯ä¸ªç±»åˆ«ä½¿ç”¨SVMåˆ†ç±»å™¨è¿›è¡Œæ‰“åˆ†
3. è¿›è¡Œç­›é€‰
4. æŸå¤±å‡½æ•°ï¼š$loU=\frac {Area Of Overlap}{Area Of Union}$

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/35.png)

Mean Average Precision (mAP)ï¼š

1. Run object detector on all test images 
2. For each category, compute Average Precision (AP) = area under Precision vs Recall Curve 

   1. For each detection (highest score to lowest score) 

      1. If it matches some GT box with IoU > 0.5, mark it as positive and eliminate the GT

      2. Otherwise mark it as negative 

      3. Plot a point on PR Curve
   2. Average Precision (AP) = area under PR curve
3. Mean Average Precision (mAP) = average of AP for each category
4. For â€œCOCO mAPâ€: Compute mAP@thresh for each IoU threshold (0.5, 0.55, 0.6, â€¦, 0.95) and take average

éæœ€å¤§æŠ‘åˆ¶(Non-Max Suppression):

1. Select the highest-scoring box 
2. Eliminate lower-scoring boxes with IoU > threshold 
3. If any boxes remain, goto 1

#### Fast R-CNN:

A two-stage object detector

- First stage: run once per image
  - Backbone network
  - RPN
- Second stage: run once per region
  - Crop features: RoI pool / align
  - Predict object class
  - Predict bbox offset

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/36.png)

#### Rol Pool

[R-CNNä¸­çš„ROIPoolã€ROIAlign](https://blog.csdn.net/qq_41021141/article/details/120617660)

#### RPN

> å‚è€ƒï¼š[è¯¦è§£RPNç½‘ç»œ](https://blog.csdn.net/weixin_42912710/article/details/119872716)
>
> RPNï¼ˆRegion Proposal Networkï¼‰æ˜¯Faster-RCNNç½‘ç»œç”¨äºæå–é¢„é€‰æ¡†ï¼ˆä¹Ÿå°±æ˜¯RCNNä¸­ä½¿ç”¨selective searchç®—æ³•è¿›è¡ŒRegion Proposalçš„éƒ¨åˆ†ï¼‰ï¼ŒR-CNNåŠFast-RCNNä¸­ä¸€ä¸ªæ€§èƒ½ç“¶é¢ˆå°±æ˜¯æå–é¢„é€‰æ¡†çš„éƒ¨åˆ†ï¼Œè€ŒRPNå¾ˆå¥½åœ°å¯¹è¿™ä¸ªéƒ¨åˆ†è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒåŸå› åœ¨äºå®ƒå°†å·ç§¯ç¥ç»ç½‘ç»œå¼•å…¥äº†è¿›æ¥ï¼Œä½¿ç”¨ç‰¹å¾æå–çš„å½¢å¼ç”Ÿæˆå‡ºé¢„é€‰æ¡†çš„ä½ç½®ä»è€Œé™ä½äº†selective searchç®—æ³•å¸¦æ¥çš„è®¡ç®—æ—¶é—´ä¸Šçš„å¼€é”€ã€‚

â€‹	å‡è®¾æˆ‘ä»¬æœ‰ä¸€å¼ å¤§å°ä¸º600Ã—800çš„å›¾åƒï¼Œåœ¨é€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å—åï¼Œè¿™å¹…è¾“å…¥å›¾åƒç¼©å°ä¸º38Ã—56çš„ç‰¹å¾å›¾ï¼Œç‰¹å¾å›¾çš„æ¯ä¸ªä½ç½®éƒ½æœ‰9ä¸ªé”šç‚¹ç›’ã€‚é‚£ä¹ˆæˆ‘ä»¬å°±æœ‰38 * 56 * 9=1192ä¸ªå»ºè®®æˆ–é”šç®±éœ€è¦è€ƒè™‘ã€‚è€Œæ¯ä¸ªé”šç®±éƒ½æœ‰ä¸¤ä¸ªå¯èƒ½çš„æ ‡ç­¾ï¼ˆå‰æ™¯æˆ–èƒŒæ™¯ï¼‰ã€‚å¦‚æœæˆ‘ä»¬æŠŠç‰¹å¾å›¾çš„æ·±åº¦å®šä¸º18ï¼ˆ9ä¸ªé”šç‚¹x 2ä¸ªæ ‡ç­¾ï¼‰ï¼Œæˆ‘ä»¬å°†ä½¿æ¯ä¸ªé”šç‚¹éƒ½æœ‰ä¸€ä¸ªæœ‰ä¸¤ä¸ªå€¼çš„å‘é‡ï¼ˆé€šå¸¸ç§°ä¸ºé¢„æµ‹å€¼ï¼‰ï¼Œä»£è¡¨å‰æ™¯å’ŒèƒŒæ™¯ã€‚å¦‚æœæˆ‘ä»¬å°†é¢„æµ‹å€¼é€å…¥softmax/logisticå›å½’æ¿€æ´»å‡½æ•°ï¼Œå®ƒå°†é¢„æµ‹æ ‡ç­¾ã€‚

### 3 Instance segmentation(å®ä¾‹åˆ†å‰²)

1. è¯­ä¹‰åˆ†å‰²çš„è¯åªéœ€è¦åˆ†å‡ºä¸åŒç±»å°±è¡Œï¼ŒåŒç±»çš„ä¸åŒä¸ªä½“ä¸éœ€è¦åˆ†ï¼Œä½†æ˜¯Instance Segmentationè¿™é‡Œåœ¨è¯­ä¹‰åˆ†å‰²çš„åŸºç¡€ä¸ŠåˆæŠŠä¸åŒçš„ç±»è¿›è¡Œäº†åˆ†å‰²
2. ç›®æ ‡æ£€æµ‹åï¼Œéœ€è¦å¯¹æ£€æµ‹çš„éƒ¨åˆ†åšè¿›ä¸€æ­¥çš„è¯­ä¹‰åˆ†å‰²

[Mask R-CNN](https://blog.csdn.net/qq_37392244/article/details/88844681):

åœ¨Faster R-CNNçš„åŸºç¡€ä¸Šæ·»åŠ äº†ä¸€ä¸ªé¢„æµ‹åˆ†å‰²maskçš„åˆ†æ”¯ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å…¶ä¸­é»‘è‰²éƒ¨åˆ†ä¸ºåŸæ¥çš„Faster-RCNNï¼Œçº¢è‰²éƒ¨åˆ†ä¸ºåœ¨Faster-RCNNç½‘ç»œä¸Šçš„ä¿®æ”¹ã€‚å°†RoI Pooling å±‚æ›¿æ¢æˆäº†RoIAlignå±‚ï¼›æ·»åŠ äº†å¹¶åˆ—çš„FCNå±‚ï¼ˆmaskå±‚ï¼‰ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/37.png)

> DeepSnake: é€šè¿‡æ·±åº¦å­¦ä¹ ç»™å‡ºè¾“å…¥åˆå§‹è½®å»“é¡¶ç‚¹éœ€è¦è°ƒæ•´çš„åç§»é‡ï¼Œä»¥å¾—åˆ°æ›´ä¸ºå‡†ç¡®çš„å®ä¾‹åˆ†å‰²ç»“æœã€‚

Panoptic segmentation: å¯¹æ¯ä¸€ä¸ªåƒç´ éƒ½åˆ†å‡ºç±»åˆ«

### 4 Human pose estimation(äººä½“å§¿æ€ä¼°è®¡)

> é€šè¿‡å®šä½ä¸€ç»„å…³é”®ç‚¹æ¥è¡¨ç¤ºäººçš„å§¿åŠ¿

å•äººï¼šRepresent joint location as the heatmap(ç°åœ¨æŠŠå…³é”®ç‚¹ç”¨çƒ­åŠ›å›¾(heat map)è¡¨ç¤ºï¼Œä¸éœ€è¦å…¨è¿æ¥å±‚ï¼ŒåŠ é€Ÿç®—æ³•å‡å°‘å‚æ•°é‡ã€‚å¯¹äºæ¯ä¸€ä¸ªå…³é”®ç‚¹è¾“å‡ºä¸€å¼ heat map)

å¤šäººï¼š

- Top-down(å‡†ç¡®)ï¼šæŠŠå¤šäººè½¬æˆå•äººï¼Œå›¾åƒåˆ†å‰²
- Bottom-up(å¿«)ï¼šå…ˆæ£€æµ‹å‡ºå›¾ä¸­æ‰€æœ‰äººçš„æ‰€æœ‰å…³é”®ç‚¹ï¼Œå†å¯¹å…³é”®ç‚¹è¿›è¡Œåˆ†ç»„ï¼Œè¿›è€Œç»„è£…æˆå¤šä¸ªäºº

> [äººä½“å§¿æ€ä¼°è®¡(Human Pose Estimation)ç»å…¸æ–¹æ³•æ•´ç†](https://zhuanlan.zhihu.com/p/104917833)

### 5 Optical flow(å…‰æµ)

> å‚è€ƒï¼š[è®¡ç®—æœºè§†è§‰--å…‰æµæ³•(optical flow)ç®€ä»‹](https://blog.csdn.net/qq_41368247/article/details/82562165)
>
> å…‰æµ(optical flow)æ˜¯ç©ºé—´è¿åŠ¨ç‰©ä½“åœ¨è§‚å¯Ÿæˆåƒå¹³é¢ä¸Šçš„åƒç´ è¿åŠ¨çš„ç¬æ—¶é€Ÿåº¦ã€‚åœ¨æ—¶é—´é—´éš”å¾ˆå°ï¼ˆæ¯”å¦‚è§†é¢‘çš„è¿ç»­å‰åä¸¤å¸§ä¹‹é—´ï¼‰æ—¶ï¼Œä¹Ÿç­‰åŒäºç›®æ ‡ç‚¹çš„ä½ç§»ã€‚

#### å…‰æµåœºï¼š

â€‹	å…‰æµåœºæ˜¯ä¸€ä¸ªäºŒç»´çŸ¢é‡åœºï¼Œå®ƒåæ˜ äº†å›¾åƒä¸Šæ¯ä¸€ç‚¹ç°åº¦çš„å˜åŒ–è¶‹åŠ¿ï¼Œå¯çœ‹æˆæ˜¯å¸¦æœ‰ç°åº¦çš„åƒç´ ç‚¹åœ¨å›¾åƒå¹³é¢ä¸Šè¿åŠ¨è€Œäº§ç”Ÿçš„ç¬æ—¶é€Ÿåº¦åœºã€‚å®ƒåŒ…å«çš„ä¿¡æ¯å³æ˜¯å„åƒç‚¹çš„ç¬æ—¶è¿åŠ¨é€Ÿåº¦çŸ¢é‡ä¿¡æ¯ã€‚

â€‹	ç ”ç©¶å…‰æµåœºçš„ç›®çš„å°±æ˜¯ä¸ºäº†ä»åºåˆ—å›¾åƒä¸­è¿‘ä¼¼è®¡ç®—ä¸èƒ½ç›´æ¥å¾—åˆ°çš„è¿åŠ¨åœºã€‚å…‰æµåœºåœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œå…‰æµåœºå¯¹åº”äºè¿åŠ¨åœºã€‚

#### FlowNet

ä¸¤éƒ¨åˆ†ï¼šç¼©å°å’Œæ”¾å¤§

- ç¼©å°(å·ç§¯)éƒ¨åˆ†
  1. ç¬¬ä¸€ç§ç¼©å°(å·ç§¯)æ–¹æ¡ˆæ˜¯æœ€æœ´ç´ çš„æ–¹æ³•çš„ï¼Œå°±æ˜¯å°†è¿™ä¸€å¯¹å›¾ç‰‡çš„é€šé“concatèµ·æ¥
  2. ç¬¬äºŒä¸­æ–¹æ¡ˆæ˜¯è¿™ä¸€å¯¹å›¾ç‰‡åˆ†å¼€å¤„ç†ï¼Œåˆ†åˆ«è¿›å…¥å·ç§¯ç½‘è·¯ï¼Œå¾—åˆ°å„è‡ªçš„ç‰¹å¾å›¾ï¼Œç„¶åå†æ‰¾åˆ°å®ƒä»¬ç‰¹å¾å›¾ä¹‹é—´çš„è”ç³»ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/39.png)

- æ”¾å¤§éƒ¨åˆ†

  ä¸€è¾¹å‘åunconvï¼Œä¸€è¾¹ç›´æ¥åœ¨å°çš„ç‰¹å¾å›¾ä¸Šé¢„æµ‹ï¼Œç„¶åæŠŠç»“æœåŒçº¿æ€§æ’å€¼ç„¶åconcatåœ¨unconvåçš„ç‰¹å¾å›¾ä¸Šï¼Œç„¶åæ¥ç€å¾€åä¼ ï¼Œé‡å¤å››æ¬¡åï¼Œå¾—åˆ°çš„é¢„æµ‹å…‰æµåˆ†è¾¨ç‡ä¾ç„¶æ˜¯è¾“å…¥çš„1/4ï¼Œå†é‡å¤ä¹‹å‰çš„æ“ä½œå·²æ²¡æœ‰å¤ªå¤šæå‡ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥åŒçº¿æ€§æ’å€¼å¾—åˆ°å’Œè¾“å…¥ç›¸åŒåˆ†è¾¨ç‡çš„å…‰æµé¢„æµ‹å›¾ã€‚

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/38.png)

### 6 Other tasks

Video classification: Use 3D CNN

Temporal action localization: Generate proposals then classify

Multi-object tracking

## Lec 11  3D Deep Learning

### 1 Feature matching

#### Super Polint: ç”¨äºæå–ç‰¹å¾ç‚¹

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/40.png)

- CNN-based detectors: Representing feature point locations by heatmaps

  ![](è®¡ç®—æœºè§†è§‰å¯¼è®º/41.png)

- CNN-based descriptors: Extract descriptors from CNN feature maps(æ¯ä¸€å¼ å›¾éƒ½æ˜¯ä¸€å±‚ç¥ç»ç½‘ç»œä¸‹çš„ç‰¹å¾å›¾ï¼Œå–æ‰€æœ‰å›¾ä¸­å¯¹åº”çš„ç‚¹åšä¸€ä¸ªå‘é‡)

  ![](è®¡ç®—æœºè§†è§‰å¯¼è®º/42.png)

#### SuperGlue: ç°åœ¨æœ€å¥½çš„æ£€æµ‹æ–¹æ³•

### 2 Object Pose Estimation

Estimate the 3D location and orientation (ä½ç½®å’Œæ–¹å‘) of an object  realtive to the camera frame.

Before that, we need to define the geometry center of the object.

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/43.png)

1. Find 3D-2D correspondences
2. Solve R and t by perspective-n-point (PnP) algorithm
3. Find 2D-3D correspondences: detecting keyponts using CNNs

### 3 Human Pose Estimation

#### Markerless MoCap: ç›¸æœºç…§å°„å…³é”®ç‚¹ï¼Œæ ‡è®°ç‰©åœ¨äººèº«ä½“ä¸Šï¼ŒMarklesså°±æ˜¯ä¸ç”¨è´´æ ‡è®°ç‰©

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/44.png)

#### Monocular 3D Human Pose Estimation: å‚æ•°åŒ–äººä½“æ¨¡å‹

Estimating 3D human pose using a single camera

Using networks to regress joint locations

### 4 Depth Estimation

> Multiview Depth Estimation: Reconstruct the dense 3D shape from a set of images and camera parameters

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/45.png)

Learned multi-view stereo

Cost volume is a 3D array that stores the errors of all pixels at all depths

### 5 Single Image to 3D

â€¢ Depth â€¢ Point Cloud â€¢ Mesh â€¢ Volume

#### Monoculer depth estimation

> Learning to guess depth from large-scale training data

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/46.png)

#### Single-view shape estimation

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/47.png)

é™¤äº†ç‚¹äº‘ï¼Œmeshç­‰æ–¹æ³•ä¹Ÿå¯åº”ç”¨ã€‚

### 6 PointNet

æŠŠç‚¹äº‘æ”¾è¿›ç¥ç»ç½‘ç»œï¼Œä»¥å‰çš„ç¥ç»ç½‘ç»œéƒ½æ˜¯å…‰æ …åŒ–çš„

æŒ‘æˆ˜1ï¼šç‚¹é¡ºåºä¸ç¡®å®šâ€”â€”æœ€åå†åšpooling

æŒ‘æˆ˜2ï¼šä½ç½®å˜åŒ–ä¸ç¡®å®šâ€”â€”ä¼°è®¡å§¿æ€

## Lec 12 Computational Photography I

### 1 High Dynamic Range Imaging (HDR)

#### Exposure:æ›å…‰

**Exposure = Gain(å¢ç›Š) x Irradiance(å…‰çº¿) x Time(æ—¶é—´)**

- Gain is controlled by the ISO 
- Irradiance is controlled by the aperture 
- Time is controlled by the shutter speed

####  Dynamic Range

The ratio between the largest and smallest values of a certain quantity.

#### HDR

äº®çš„åœ°æ–¹å’Œæš—çš„åœ°æ–¹èƒ½åŒæ—¶æ‹æ¸…æ¥š

å®ç°æ–¹æ³•ï¼š

- Exposure bracketing: Capture multiple LDR images at different exposures(ä¸€æ¬¡æ‹å¾ˆå¤šå¼ )
- Merging: Combine them into a single HDR image(ç„¶ååˆåœ¨ä¸€èµ·)

#### Merge

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/48.png)

#### Tone mapping

Display the HDR image (12-bit) on a SDR (standard dynamic range, 8-bit) device.

> Gamma compression

X â†’ aX^Î³^, applied independently on R, G, B intend to keep more details on each parts.

- Î³ < 1:  keep more datails on dart parts

- Î³ > 1: keep more datails on light parts

### 2 Deblurring

#### Reason

- Defocus: the subject is not in the depth of view
- Motion blur: moving subjects or unstable camera
- â€¦â€¦

#### Modeling Image Blur

The blurring process can be described by convolution. H is called blur kernel.

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/49.png)

Deblurring = Deconvolution

- NBID: Non-blind image deconvolution, the blur kernel is known.
- BID: Blind image deconvolution, the blur kernel is also unknown.

#### NBID

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/50.png)

- G: The captured image (known) 
- F: Image to be solved (unknown) 
- H: Blur kernel (known)

Inverse Filter: will also amplify noise

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/51.png)

Wiener Filter: Suppress high frequency when reverse filtering

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/52.png)

Deconvolution by optimization

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/53.png)

Deconvolution is ill-posed, these are several sets of solutions have the same MSE.

Objective function = likelihood function + regular term

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/54.png)

#### BID

Blur kernel is non-negative and sparse.

Optimized objective function:

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/55.png)



### 3 Colorization(é»‘ç™½å˜å½©è‰²)

#### Sample-based colorization

æŠŠä¸€å¼ ç…§ç‰‡ä¸Šçš„é¢œè‰²è¿ç§»åˆ°å¦ä¸€å¼ ç…§ç‰‡ï¼Œä¸»è¦ä»»åŠ¡æ˜¯åƒç´ åŒ¹é…

#### Interactive colorization

ç»™å‡ºçº¿æ¡å¤§è‡´çš„é¢œè‰²(user-guided)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/56.png)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/57.png)

Constraint: User-specified colors of brushed pixels keep unchanged

#### Generative Adversarial Network (GAN)

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/58.png)

D can be viewed as a loss function to train Gï¼š

- Called adversarial loss 
- Learned instead of being hand-designed 
- Can be applied to any image synthesis tasks

### 4 Super Resolution

Super Resolution using GAN

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/59.png)

## Lec 13 Computational Photography II

### 1 Image-based Rendering

Rendering: from 3D models to images

Image-based: åŸºäºç…§ç‰‡è¿›è¡Œæ¸²æŸ“

#### Light Fields

The plenoptic function (7D) depicts light rays passing through.

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/60.png)

#### Multi-Plane Image (MPI)

A set of front-parallel planes at a fixed range of depths.

Each plane encodes an RGB color image and an alpha/transparency map Î±~d~.

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/61.png)

#### NeRF

![](è®¡ç®—æœºè§†è§‰å¯¼è®º/62.png)

### 2 Neural Rendering

#### Pose Transfer & Garment Transfer

> Input: Image(s) of a person
>
> Output: Synthesised images of the persion in different poses (Pose Transfer),  or with different clothing (Garment Transfer).

Method:

- Use parametric mesh (SMPL) to represent body pose and shape 
- Use high-dimensional UV texture map to encode appearance 
- Transfer the pose and appearance
